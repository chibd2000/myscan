# coding=utf-8
# @Author   : zpchcbd HG team
# @Time     : 2021-09-14 22:56

from core.data import gLogger
from exploit.scripts import BaseScript
from core.myenums import BugType, BugLevel
from core.parser.urlparser import UrlParser
from core.request.asynchttp import *
from ipaddress import ip_address
from hurry.filesize import size

# python3 batch.py -m exploit.scripts.Info.backup.get_web_backup_file -fs "body=\"/interlib/common/\"" -cs

class Script(BaseScript):
    name = 'Get Web Backup'

    def __init__(self, target):
        super().__init__()
        # 漏洞目标
        self.target = target
        # 漏洞等级
        self.bug_level = BugLevel.HIGH
        # 类型
        self.bug_type = BugType.BACKUP
        # 编号
        self.bug_number = ''
        # 来源
        self.bug_refer = ''
        # 特定路径判断
        self.detect_path_list = []
        # exec
        self.exec_path_list = []
        # 相关信息
        self.info = ''

    def _generate_backup_path(self, target):
        ip_flag = True

        try:
            ip_address(target.split('//')[1])
        except ValueError:
            ip_flag = False
        except IndexError:
            pass

        sub_domain_names = []
        # root_domain_name = ''
        if target is not None and ip_flag is False:
            url_parser = UrlParser(target)
            sub_domain_names.append(url_parser.parse_result.netloc)
            sub_domain_names.append(url_parser.extract_result.domain)
            # sub_domain_names = urlparse(target).netloc.split('.')[-1]
        file_prefixs = list(set(['1', '127.0.0.1', '2018', '2019', '2020', '2021', '2022', 'admin', 'archive', 'backup', 'bak', 'data', 'database', 'db', 'local', 'localhost', 'new', 'old', 'site', 'web', 'website', 'www', 'wwwroot', 'root'] + sub_domain_names))
        # file_prefixs = list(set(['www','wwwroot','root'] + sub_domain_names))
        file_suffixs = ['zip', 'rar', 'tar.gz', 'tgz', 'tar.bz2', 'tar', 'jar', 'war', '7z', 'bak', 'sql', 'gz', 'sql.gz', 'tar.tgz']
        for file_prefix in file_prefixs:
            for file_suffix in file_suffixs:
                yield '/' + file_prefix + '.' + file_suffix

    async def detect(self):
        url = f'http://{self.target}/' if self.target.startswith(('http:', 'https:')) is False else f'{self.target}/'
        self.detect_path_list = list(self._generate_backup_path(self.target))
        async with aiohttp.ClientSession() as session:
            for detect_path in self.detect_path_list:
                url = f'http://{self.target}{detect_path}' if self.target.startswith(('http:', 'https:')) is False else f'{self.target}{detect_path}'
                async with session.get(url=url, headers=self.headers, timeout=10, allow_redirects=False, verify_ssl=False) as response:
                    if (response.status == 200) and ('html' not in response.headers.get('Content-Type', '')) and ('xml' not in response.headers.get('Content-Type', '')) and ('json' not in response.headers.get('Content-Type', '')) and ('javascript' not in response.headers.get('Content-Type', '')):
                        bak_size = size(int(response.headers.get('Content-Length', 0)))
                        if int(bak_size[0:-1]) > 0:
                            self.flag = True
                            gLogger.myscan_info('[{} {} size:{}] {}'.format(self.name, BugType.FINGER, bak_size, url))
                            return {'name': '{} size:{} {}'.format(self.name, bak_size, BugType.FINGER), 'url': url, 'software': self.name}

    def test_path(self):
        print(list(self._generate_backup_path(self.target)))

if __name__ == '__main__':
    asyncio.get_event_loop().run_until_complete(Script('http://aaa.bbb.baidu.com:8080/').attack())
