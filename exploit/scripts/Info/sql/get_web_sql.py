# coding=utf-8
# @Author   : zpchcbd HG team
# @Time     : 2021-09-14 22:56

from core.data import gLogger
from core.constant import ERROR_FLAG
from core.parser.urlparser import UrlParser
from core.utils.differ import DifferentChecker
from exploit.scripts import BaseScript
from core.myenums import BugType, BugLevel
from core.setting import SQL_HTML_FLAG_SIGN, SQL_SHTML_FLAG_SIGN
from core.request.asynchttp import *
from bs4 import BeautifulSoup
import re

def get_current_url_list(links, suffix_compile):
    current_url_list = []
    for link in links:
        a_link = link.get('href', '')
        if a_link:
            _ = suffix_compile.search(str(a_link))
            if _ is None:
                current_url_list.append(str(a_link))  # 是的话 那么添加到result列表中
    return current_url_list


class ParamSpider:
    """
    对于相关的动态脚本和js参数资产自己封装到这个类中进行使用
    write in 2021.11.21 12.03 @zpchcbd
    """
    def __init__(self):
        self.source = 'ParamSpider'
        self.reqTimeout = 15
        self.headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36'}

    # learn from jsFinder / langzi.fun
    async def get_dynamic_script_links(self, session, url, linkList):
        """实现动态脚本参数的获取"""
        script_links = []
        html_links = []
        urlparser = UrlParser(url)
        """分别识别伪静态和动态链接"""
        for link in linkList:  # 再进行二次判断是不是子域名 这次的判断有三种情况
            if link.startswith('http') and '://' in link and urlparser.subdomain in link and '.js?' not in link and '.min.js' not in link:
                # http://www.baidu.com
                if '?' in link and '=' in link:
                    # result_links.append(rurl)
                    script_links.append(link.strip())
                if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                    if '?' not in link:
                        # result_links.append(rurl)
                        html_links.append(link.strip())

            if 'http' not in link and urlparser.subdomain in link and '.js?' not in link and '.min.js' not in link:
                if 'www' in url:
                    if 'www' in link:
                        if '?' in link and '=' in link:
                            script_links.append(urlparser.scheme + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//','').replace(':', ''))
                        if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                            if '?' not in link:
                                # result_links.append(rurl)
                                html_links.append(urlparser.scheme + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//','').replace(':', ''))
                    else:
                        if '?' in link and '=' in link:
                            script_links.append(urlparser.scheme + 'www.' + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//','').replace(':', ''))
                        if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                            if '?' not in link:
                                html_links.append(urlparser.scheme + 'www.' + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//', '').replace(':', ''))
                else:
                    if '?' in link and '=' in link:
                        script_links.append(urlparser.scheme + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//', '').replace(':', ''))
                    if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                        if '?' not in link:
                            html_links.append(urlparser.scheme + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//','').replace(':', ''))

            if 'http' not in link and urlparser.subdomain not in link and ':' not in link and '//' not in link and '.js?' not in link and '.min.js' not in link:
                # /sttd/xhm/
                if '?' in link and '=' in link:
                    script_links.append(urlparser.scheme + urlparser.subdomain + '/' + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//', '').replace(':', ''))
                if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                    if '?' not in link:
                        html_links.append(urlparser.scheme + urlparser.subdomain + '/' + link.lstrip('/').lstrip('.').rstrip('/').rstrip('.').replace('//', '').replace(':', ''))

            if link.startswith('://') and 'http' not in link and urlparser.subdomain in link and '.js?' not in link and '.min.js' not in link:
                if '?' in link and '=' in link:
                    script_links.append(urlparser.scheme + link.replace('://', ''))
                if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                    if '?' not in link:
                        html_links.append(urlparser.scheme + link.replace('://', ''))

            if link.startswith('//') and urlparser.subdomain in link and '.js?' not in link and '.min.js' not in link:
                # //order.jd.com/center/list.action
                if '?' in link and '=' in link:
                    script_links.append(urlparser.scheme + link.replace('//', ''))
                if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                    if '?' not in link:
                        html_links.append(urlparser.scheme + link.replace('//', ''))

            if '//' in link and link.startswith('http') and urlparser.subdomain in link and '.js?' not in link and '.min.js' not in link:
                # http // domain 都在
                # https://www.yamibuy.com/cn/search.php?tags=163
                # http://news.hnu.edu.cn/zhyw/2017-11-11/19605.html
                if '?' in link and '=' in link:
                    script_links.append(link.strip())
                if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                    if '?' not in link:
                        html_links.append(link.strip())
            # //wmw.dbw.cn/system/2018/09/25/001298805.shtml
            if 'http' not in link and urlparser.subdomain in link and '.js?' not in link and '.min.js' not in link:
                # http 不在    domain 在
                if '?' in link and '=' in link:
                    script_links.append(urlparser.scheme + link.lstrip('/').lstrip('.').strip().lstrip('/'))
                if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                    if '?' not in link:
                        html_links.append(urlparser.scheme + link.lstrip('/').lstrip('.').strip().lstrip('/'))

            # /chanpin/2018-07-12/3.html"
            if 'http' not in link and urlparser.subdomain not in link and '.js?' not in link and '.min.js' not in link:
                # http 不在  domain 不在
                if '?' in link and '=' in link:
                    script_links.append(urlparser.scheme + urlparser.subdomain.strip() + '/' + link.strip().lstrip('/').lstrip('.').lstrip('/'))
                if '.html' in link or '.shtml' in link or '.htm' in link or '.shtm' in link:
                    if '?' not in link:
                        html_links.append(urlparser.scheme + urlparser.subdomain.strip() + '/' + link.strip().lstrip('/').lstrip('.').lstrip('/'))

        script_list = self._flush_link(script_links)
        html_list = self._flush_link(html_links)
        return script_list, html_list

    def _flush_link(self, links):
        """匹配相似度清洗数据 write in 2021.11.24 15.48"""
        result_list = []
        link_index = 0
        while link_index < len(links):
            current = links[link_index]
            good_index_list = DifferentChecker.get_close_match_index(current, links, n=10000, cutoff=0.9)
            current_result_list = []
            for index in reversed(sorted(good_index_list)):
                current_result_list.append(links[index])
                del links[index]
            result_list.append(current_result_list[0])
            link_index += 1
        return result_list

    # learn from jsfinder
    async def get_javascript_links(self, session, domain, text):
        """实现javascript参数的获取 特征"/static/js/app" "/static/js/main"  """
        jsUrlList = self.extract_URL(text)
        for aJs in jsUrlList:
            pass

def check_error_inject_keyword(text):
    for k, v in ERROR_FLAG.items():
        if k in text:
            return True, v
        else:
            return None, None

class Script(BaseScript):
    name = 'Get Web Sql Injection'

    def __init__(self, target):
        super().__init__()
        # 漏洞目标
        self.target = target
        # 漏洞等级
        self.bug_level = BugLevel.HIGH
        # 类型
        self.bug_type = BugType.SQLINJECTION
        # 编号
        self.bug_number = ''
        # 来源
        self.bug_refer = ''
        # 特定路径判断
        self.detect_path_list = ['/']
        # exec
        self.exec_path_list = []
        # 相关信息
        self.info = 'sql injection check'
        # param spider
        self.param_spider = ParamSpider()
        # suffix compile
        self.suffix_compile = re.compile(r'\.(gz|zip|rar|iso|pdf|txt|3ds|3g2|3gp|7z|DS_Store|a|aac|adp|ai|aif|aiff|apk|ar|asf|au|avi|bak|bin|bk|bmp|btif|bz2|cab|caf|cgm|cmx|cpio|cr2|dat|deb|djvu|dll|dmg|dmp|dng|doc|docx|dot|dotx|dra|dsk|dts|dtshd|dvb|dwg|dxf|ear|ecelp4800|ecelp7470|ecelp9600|egg|eol|eot|epub|exe|f4v|fbs|fh|fla|flac|fli|flv|fpx|css|fst|fvt|g3|gif|gz|h261|h263|h264|ico|ief|image|img|ipa|iso|jar|jpg|jpeg|jpgv|jpm|jxr|ktx|lvp|lz|lzma|lzo|m3u|m4a|m4v|mar|mdi|mid|mj2|mka|mkv|mmr|mng|mov|movie|mp3|mp4|mp4a|mpeg|mpg|mpga|mxu|nef|npx|o|oga|ogg|ogv|otf|pbm|pcx|pdf|pea|pgm|pic|png|pnm|ppm|pps|ppt|pptx|ps|psd|pya|pyc|pyo|pyv|qt|rar|ras|raw|rgb|rip|rlc|rz|s3m|s7z|scm|scpt|sgi|shar|sil|smv|so|sub|swf|tar|tbz2|tga|tgz|tif|tiff|tlz|ts|ttf|uvh|uvi|uvm|uvp|uvs|uvu|viv|vob|war|wav|wax|wbmp|wdp|weba|webm|webp|whl|wm|wma|wmv|wmx|woff|woff2|wvx|xbm|xif|xls|xlsx|xlt|xm|xpi|xpm|xwd|xz|z|zip|zipx)|javascript|:;|#|%')
        # param link list
        self.param_link_list = list()
        # payload
        self.sql_payload_list = ['-0', '%25%27', '%2caaaaaaa', '%27', '%27)', '%22)', '%5c', '%df%27']

    def parse(self, web_param_list):
        """
        分析当前URL，对可能存在的注入点进行替换注入标记
        情况有如下：
        1、HTML的情况, 这种情况可能会存在多种情况，但是自己这里的话就简单处理
        2、单参数的情况, 这种情况只替换一次
        3、多参数的情况, 这种情况替换多次
        """
        fina_web_param_link = []
        for web_param_link in web_param_list:
            if '.htm' in web_param_link or '.shtm' in web_param_link:
                for payload in self.sql_payload_list:
                    new_link = web_param_link.replace('.htm', payload + SQL_HTML_FLAG_SIGN).replace('.shtm', payload + SQL_SHTML_FLAG_SIGN)
                    fina_web_param_link.append(new_link)
                continue
            link = UrlParser(web_param_link).parse_result
            for _ in link.query.split('&'):
                param_name, param_value = _.split('=')
                for payload in self.sql_payload_list:
                    new_param_value = param_value + payload
                    new_param = param_name + '=' + new_param_value
                    new_link = web_param_link.replace(_, new_param)
                    fina_web_param_link.append(new_link)

        return fina_web_param_link

    async def detect(self):
        domain = UrlParser(self.target).parse_result.netloc
        async with aiohttp.ClientSession() as session:
            for detect_path in self.detect_path_list:
                url = f'http://{self.target}{detect_path}' if self.target.startswith(('http:', 'https:')) is False else f'{self.target}{detect_path}'
                text = await AsyncFetcher.fetch(session=session, url=url, headers=self.headers, timeout=self.req_timeout)
                a_link_list = BeautifulSoup(text, 'lxml').findAll('a')
                result = list(set(get_current_url_list(a_link_list, self.suffix_compile)))
                if result:
                    script_fina_links, html_fina_links = await self.param_spider.get_dynamic_script_links(session, url, result)
                    if script_fina_links:
                        for script_link in script_fina_links:
                            if domain in script_link:
                                self.param_link_list.append(script_link)
                    if html_fina_links:
                        for html_link in html_fina_links:
                            if domain in html_link:
                                self.param_link_list.append(html_link)

    async def exec(self):
        self.exec_path_list = self.parse(self.param_link_list)
        async with aiohttp.ClientSession() as session:
            for exec_path in self.exec_path_list:
                text = await AsyncFetcher.fetch(session=session, url=exec_path, headers=self.headers, timeout=self.req_timeout)
                if text is not None:
                    flag, database_type = check_error_inject_keyword(text)
                    if flag:
                        gLogger.myscan_info('[{} {}] {}'.format(self.name, self.bug_type, exec_path))
                        self.vul_list.append({'name': 'SQL Injection', 'url': exec_path, 'software': database_type})

    async def attack(self):
        await self.detect()
        await self.exec()
        return self.vul_list

if __name__ == '__main__':
    t = asyncio.get_event_loop().run_until_complete(Script('http://testaspnet.vulnweb.com').attack())
